{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2c96cca5-d05a-4ceb-91cf-117308faeab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset import SingleStage  # Adjust if your dataset class is named differently\n",
    "from utils import calculate_perplexity, calculate_distinct_n\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5514670a-3851-4d81-8302-fa9083103cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.yaml') as file:\n",
    "    config = yaml.full_load(file)\n",
    "\n",
    "training_args = config['training_args']\n",
    "\n",
    "# Checkpoint directory\n",
    "checkpoint_dir = os.path.join(config['model']['save_path'], 'single_stage')\n",
    "model_path = os.path.join(checkpoint_dir, 'model.safetensors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f9995f4f-6e23-47b6-a3eb-e60771123583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from checkpoint...\n"
     ]
    }
   ],
   "source": [
    "decoder_name = config['model']['decoder']\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(decoder_name, padding_side=\"left\")\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    print(\"Loading model from checkpoint...\")\n",
    "    model = GPT2LMHeadModel.from_pretrained(checkpoint_dir, use_safetensors=True)\n",
    "else:\n",
    "    print(\"Initializing new model...\")\n",
    "    raise FileNotFoundError(f\"Checkpoint not found in {checkpoint_dir}. Ensure the checkpoint exists before resuming.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e8cff744-df4d-4558-9ba6-bb9421d464b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "model.resize_token_embeddings(len(tokenizer), mean_resizing=False)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7ec04aea-e601-4fc5-b508-2cbfe4fe3a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SingleStage(config['dataset']['train_path'], tokenizer)\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5b586373-086a-4f98-8ffa-1a4972ecba54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs Shape: torch.Size([1, 128])\n"
     ]
    }
   ],
   "source": [
    "sample_index = 0  # Change this index to sample a different example\n",
    "sample_data = train_dataset[sample_index]\n",
    "\n",
    "# Ensure 'input_ids' and 'attention_mask' are in the correct format\n",
    "if 'input_ids' in sample_data and 'attention_mask' in sample_data:\n",
    "    input_ids = sample_data['input_ids'].to(device)  # No need to unsqueeze, it's already 2D\n",
    "    attention_mask = sample_data['attention_mask'].to(device)  # Move to device\n",
    "else:\n",
    "    raise KeyError(\"'input_ids' or 'attention_mask' not found in sample_data\")\n",
    "\n",
    "# Check the shape of input_ids\n",
    "print(\"Input IDs Shape:\", input_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "388821a9-47cd-4145-aeba-099865cb37ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # Use max_new_tokens to specify how many tokens to generate\n",
    "    generated_ids = model.generate(input_ids, attention_mask=attention_mask, max_new_tokens=20, num_return_sequences=1, pad_token_id=tokenizer.pad_token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "87dfff25-6eda-475e-9f9c-b94581257c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "68b45d3d-9d2b-4a55-b5ff-3f20584fe309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text:\n",
      " [ WP ] You 've finally managed to discover the secret to immortality . Suddenly , Death appears before you , hands you a business card , and says , `` When you realize living forever sucks , call this number , I 've got a job offer for you . '' <SEP> So many times have I walked on ruins , the remainings of places that I loved and got used to.. At first I was scared , each time I could feel my city , my current generation collapse , break into the black hole that thrives within it , I could feel humanity , the way I 'm able to feel my body.. After a\n",
      "Generated Text:\n",
      " [ WP ] You 've finally managed to discover the secret to immortality . Suddenly , Death appears before you , hands you a business card , and says , `` When you realize living forever sucks , call this number , I 've got a job offer for you . '' <SEP> So many times have I walked on ruins , the remainings of places that I loved and got used to.. At first I was scared , each time I could feel my city , my current generation collapse , break into the black hole that thrives within it , I could feel humanity , the way I 'm able to feel my body.. After a while I realized that I was n't dead , I was n't dying , I was just floating\n"
     ]
    }
   ],
   "source": [
    "print(\"Input Text:\\n\", tokenizer.decode(input_ids[0], skip_special_tokens=True))\n",
    "print(\"Generated Text:\\n\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38c63dc-3dc2-426d-9f42-f5ce9c1d65c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
